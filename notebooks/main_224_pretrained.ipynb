{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b233746a",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f473e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kagglehub\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "import random\n",
    "import optuna\n",
    "from optuna.trial import TrialState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25f35ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = kagglehub.dataset_download(\"sidharkal/sports-image-classification\")\n",
    "\n",
    "# print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "151f49eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data\"\n",
    "images_dir = \"../data/dataset/\"\n",
    "train_dir = images_dir + \"train/\"\n",
    "\n",
    "badminton_train_dir = train_dir + \"Badminton/\"\n",
    "tennis_train_dir = train_dir + \"Tennis/\"\n",
    "cricket_train_dir = train_dir + \"Cricket/\"\n",
    "soccer_train_dir = train_dir + \"Soccer/\"\n",
    "swimming_train_dir = train_dir + \"Swimming/\"\n",
    "karate_train_dir = train_dir + \"Karate/\"\n",
    "wrestling_train_dir = train_dir + \"Wrestling/\"\n",
    "\n",
    "test_dir = images_dir + \"test/\"\n",
    "\n",
    "badminton_test_dir = test_dir + \"Badminton/\"\n",
    "tennis_test_dir = test_dir + \"Tennis/\"\n",
    "cricket_test_dir = test_dir + \"Cricket/\"\n",
    "soccer_test_dir = test_dir + \"Soccer/\"\n",
    "swimming_test_dir = test_dir + \"Swimming/\"\n",
    "karate_test_dir = test_dir + \"Karate/\"\n",
    "wrestling_test_dir = test_dir + \"Wrestling/\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cd7881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1c22e10e4b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117566cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shutil.copytree(path, data_dir, dirs_exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcce4e74",
   "metadata": {},
   "source": [
    "## Organizing Data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfb6dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the train.csv , test.csv\n",
    "train_df = pd.read_csv(images_dir +\"/train.csv\")\n",
    "test_df = pd.read_csv(images_dir +\"/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c428c941",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9695389",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6485e771",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.set_index(\"image_ID\", inplace=True), test_df.set_index(\"image_ID\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3195bbec",
   "metadata": {},
   "source": [
    "### Moving data to be per label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082ebf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train_df[\"label\"].unique()\n",
    "\n",
    "for label in labels:\n",
    "    os.makedirs(train_dir + label, exist_ok=True)\n",
    "    os.makedirs(test_dir + label, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06c754f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0b7d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc['7c225f7b61.jpg']['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676fe4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d29e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_df)):\n",
    "    image_id = train_df.index[i]\n",
    "    label = train_df['label'][i]\n",
    "    old_path = train_dir + image_id\n",
    "    new_path = train_dir + label + \"/\" + image_id\n",
    "    if os.path.exists(old_path):\n",
    "        shutil.move(old_path, new_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cbdc1f",
   "metadata": {},
   "source": [
    "### Labeling Test data as it was unlabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ccbf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "classes = ['Badminton', 'Cricket', 'Tennis', 'Swimming', 'Soccer', 'Wrestling', 'Karate']\n",
    "\n",
    "def classify_with_clip(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(text=classes, images=image, return_tensors=\"pt\", padding=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    outputs = model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image \n",
    "    probs = logits_per_image.softmax(dim=1)\n",
    "    pred = probs.argmax()\n",
    "    return classes[pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b70337",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(test_df))):\n",
    "    image_id = test_df.index[i]\n",
    "    image_path = test_dir + image_id\n",
    "    if not os.path.exists(image_path):\n",
    "        continue\n",
    "    label = classify_with_clip(image_path)\n",
    "    test_df.at[image_id, 'label'] = label\n",
    "    new_path = test_dir + label + \"/\" + image_id\n",
    "    if os.path.exists(image_path):\n",
    "        shutil.move(image_path, new_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3361f6",
   "metadata": {},
   "source": [
    "## Statistics from the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb26c9a3",
   "metadata": {},
   "source": [
    "### Checking distribution of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fca6111",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classes = ['Badminton', 'Cricket', 'Tennis', 'Swimming', 'Soccer', 'Wrestling', 'Karate']\n",
    "\n",
    "train_dirs = [train_dir + cls + \"/\" for cls in classes]\n",
    "test_dirs = [test_dir + cls + \"/\" for cls in classes]\n",
    "\n",
    "train_counts = [len(os.listdir(d)) if os.path.exists(d) else 0 for d in train_dirs]\n",
    "test_counts = [len(os.listdir(d)) if os.path.exists(d) else 0 for d in test_dirs]\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "axs[0].bar(classes, train_counts, color='skyblue')\n",
    "axs[0].set_title('Train Class Distribution')\n",
    "axs[0].set_xlabel('Class')\n",
    "axs[0].set_ylabel('Number of Images')\n",
    "axs[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "axs[1].bar(classes, test_counts, color='lightgreen')\n",
    "axs[1].set_title('Test Class Distribution')\n",
    "axs[1].set_xlabel('Class')\n",
    "axs[1].set_ylabel('Number of Images')\n",
    "axs[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922f4e92",
   "metadata": {},
   "source": [
    "### Per class statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb80b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(classes))  \n",
    "width = 0.35  \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "rects1 = ax.bar(x - width/2, train_counts, width, label='Train', color='skyblue')\n",
    "rects2 = ax.bar(x + width/2, test_counts, width, label='Test', color='lightgreen')\n",
    "\n",
    "ax.set_ylabel('Number of Images')\n",
    "ax.set_title('Per-Class Distribution: Train vs Test')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(classes, rotation=45)\n",
    "ax.legend()\n",
    "\n",
    "def annotate_bars(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height}',\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  \n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "annotate_bars(rects1)\n",
    "annotate_bars(rects2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701ebaa8",
   "metadata": {},
   "source": [
    "### Pixel Value Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd12e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_brightness = {cls: [] for cls in classes}\n",
    "\n",
    "for cls, folder in zip(classes, train_dirs):\n",
    "    if not os.path.exists(folder):\n",
    "        continue\n",
    "    for img_file in os.listdir(folder):\n",
    "        img_path = os.path.join(folder, img_file)\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"L\")  \n",
    "            img_arr = np.array(img)\n",
    "            mean_brightness = img_arr.mean()\n",
    "            class_brightness[cls].append(mean_brightness)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {img_path}: {e}\")\n",
    "\n",
    "# Plot distributions\n",
    "fig, axs = plt.subplots(len(classes), 1, figsize=(8, len(classes)*3))\n",
    "\n",
    "for idx, cls in enumerate(tqdm(classes)):\n",
    "    axs[idx].hist(class_brightness[cls], bins=30, color='skyblue', edgecolor='black', density=True)\n",
    "    axs[idx].set_title(f'Pixel Distribution: {cls}')\n",
    "    axs[idx].set_xlabel('Mean Pixel Values')\n",
    "    axs[idx].set_ylabel('Density')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74e39df",
   "metadata": {},
   "source": [
    "- Pixels values are distributed well across all the images meaning that the images are not too dark or too bright.\n",
    "- Thus images is considered to be well exposed and not too dark or too bright so little noise are added."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8294e3a",
   "metadata": {},
   "source": [
    "## Showing some images per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed295ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(classes), 2, figsize=(8, len(classes) * 3))\n",
    "\n",
    "for row_idx, (cls, folder) in enumerate(zip(classes, train_dirs)):\n",
    "    if not os.path.exists(folder):\n",
    "        continue\n",
    "    images = [f for f in os.listdir(folder) if f.lower().endswith(('jpg', 'jpeg', 'png'))]\n",
    "    selected_images = images[:2]  \n",
    "    for col_idx in range(2):\n",
    "        if col_idx < len(selected_images):\n",
    "            img_path = os.path.join(folder, selected_images[col_idx])\n",
    "            img = Image.open(img_path)\n",
    "            axs[row_idx, col_idx].imshow(img)\n",
    "            axs[row_idx, col_idx].axis('off')\n",
    "            if col_idx == 0:\n",
    "                axs[row_idx, col_idx].set_title(f\"{cls} - Sample 1\")\n",
    "            else:\n",
    "                axs[row_idx, col_idx].set_title(f\"{cls} - Sample 2\")\n",
    "        else:\n",
    "            axs[row_idx, col_idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef71177e",
   "metadata": {},
   "source": [
    "## Dataset class and data manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8e9deb",
   "metadata": {},
   "source": [
    "### Dataset class 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef6da8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['Badminton', 'Cricket', 'Tennis', 'Swimming', 'Soccer', 'Wrestling', 'Karate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9eca89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset1(Dataset):\n",
    "    def __init__(self, root_dir, classes, transform=None, is_train=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Directory with all the class folders\n",
    "            classes (list): List of class names (subfolder names)\n",
    "            transform (callable, optional): Optional transform to be applied on a sample\n",
    "            is_train (bool): Whether this is training data or not\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.classes = classes\n",
    "        self.transform = transform\n",
    "        self.is_train = is_train\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n",
    "        self.samples = []\n",
    "\n",
    "        # Default transforms if none provided\n",
    "        if self.transform is None:\n",
    "            if is_train:\n",
    "                self.transform = T.Compose([\n",
    "                    T.RandomResizedCrop(224), # Resize to 128x128\n",
    "                    # T.RandomHorizontalFlip(),\n",
    "                    # T.RandomRotation(15),\n",
    "                    T.ToTensor(),\n",
    "                ])\n",
    "            else:\n",
    "                self.transform = T.Compose([\n",
    "                    T.Resize(224),\n",
    "                    T.CenterCrop(224), # Resize to 128x128\n",
    "                    T.ToTensor(),\n",
    "                ])\n",
    "\n",
    "        for idx, cls in enumerate(classes):\n",
    "            class_folder = os.path.join(root_dir, cls)\n",
    "            if not os.path.isdir(class_folder):\n",
    "                continue\n",
    "            for img_name in os.listdir(class_folder):\n",
    "                if img_name.lower().endswith(('jpg', 'jpeg', 'png')):\n",
    "                    img_path = os.path.join(class_folder, img_name)\n",
    "                    self.samples.append((img_path, idx))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx, retry=0):\n",
    "        img_path, label = self.samples[idx]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, label\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {str(e)}\")\n",
    "            if retry < 3:\n",
    "                return self.__getitem__(random.randint(0, len(self)-1), retry=retry+1)\n",
    "            else:\n",
    "                raise RuntimeError(\"Too many failed image loads.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e63159",
   "metadata": {},
   "source": [
    "### Dataset class 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1b2130",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset2(Dataset):\n",
    "    def __init__(self, root_dir, classes, transform=None, is_train=True, split_ratio=0.8, seed=42):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Directory with all the class folders\n",
    "            classes (list): List of class names (subfolder names)\n",
    "            transform (callable, optional): Optional transform to be applied on a sample\n",
    "            is_train (bool): Whether this is training data or not\n",
    "            split_ratio (float): Ratio for training data (default is 0.8)\n",
    "            seed (int): Seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.classes = classes\n",
    "        self.transform = transform\n",
    "        self.is_train = is_train\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n",
    "        self.samples = []\n",
    "\n",
    "        all_samples = []\n",
    "        for idx, cls in enumerate(classes):\n",
    "            class_folder = os.path.join(root_dir, cls)\n",
    "            if not os.path.isdir(class_folder):\n",
    "                continue\n",
    "            for img_name in os.listdir(class_folder):\n",
    "                if img_name.lower().endswith(('jpg', 'jpeg', 'png')):\n",
    "                    img_path = os.path.join(class_folder, img_name)\n",
    "                    all_samples.append((img_path, idx))\n",
    "\n",
    "        # Shuffle and split once\n",
    "        random.seed(seed)\n",
    "        random.shuffle(all_samples)\n",
    "        split_point = int(len(all_samples) * split_ratio)\n",
    "        if is_train:\n",
    "            self.samples = all_samples[:split_point]\n",
    "        else:\n",
    "            self.samples = all_samples[split_point:]\n",
    "\n",
    "        # Set default transforms if not provided\n",
    "        if self.transform is None:\n",
    "            if is_train:\n",
    "                self.transform = T.Compose([\n",
    "                    T.RandomResizedCrop(224),\n",
    "                    T.ToTensor(),\n",
    "                ])\n",
    "            else:\n",
    "                self.transform = T.Compose([\n",
    "                    T.Resize(224),\n",
    "                    T.CenterCrop(128),\n",
    "                    T.ToTensor(),\n",
    "                ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx, retry=0):\n",
    "        img_path, label = self.samples[idx]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, label\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {str(e)}\")\n",
    "            if retry < 3:\n",
    "                return self.__getitem__(random.randint(0, len(self)-1), retry=retry+1)\n",
    "            else:\n",
    "                raise RuntimeError(\"Too many failed image loads.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896485b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['Badminton', 'Cricket', 'Tennis', 'Swimming', 'Soccer', 'Wrestling', 'Karate']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511930a9",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cd9f91",
   "metadata": {},
   "source": [
    "# Utils for Pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3e17a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_all_but_last_n(model, n=2):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Get all modules with parameters\n",
    "    modules_with_params = [m for m in model.modules() if any(p.requires_grad is False for p in m.parameters())]\n",
    "\n",
    "    # Unfreeze last n modules with parameters\n",
    "    for module in modules_with_params[-n:]:\n",
    "        for param in module.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def print_trainable_params(model):\n",
    "    print(\"Trainable Parameters:\")\n",
    "    total = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            num_params = param.numel()\n",
    "            # print(f\"{name}: {num_params}\")\n",
    "            total += num_params\n",
    "    print(f\"Total Trainable Parameters: {total}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069d2c9c",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c243897e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "    if model_name == \"resnet18\":\n",
    "        resnet18 = models.resnet18(weights='DEFAULT')\n",
    "        resnet18.fc = nn.Linear(resnet18.fc.in_features, 7)  # Change the output layer to match the number of classes\n",
    "        resnet18 = freeze_all_but_last_n(resnet18, 2)  # Freeze all but the last 2 layers\n",
    "        resnet18 = resnet18.to(device)\n",
    "        return resnet18\n",
    "\n",
    "    elif model_name == \"resnet34\":\n",
    "        resnet34 = models.resnet34(weights='DEFAULT')\n",
    "        resnet34.fc = nn.Linear(resnet34.fc.in_features, 7)\n",
    "        resnet34 = freeze_all_but_last_n(resnet34, 2)\n",
    "        resnet34 = resnet34.to(device)\n",
    "        return resnet34\n",
    "    \n",
    "    elif model_name == \"resnet50\":\n",
    "        resnet50 = models.resnet50(weights='DEFAULT')\n",
    "        resnet50.fc = nn.Linear(resnet50.fc.in_features, 7)\n",
    "        resnet50 = freeze_all_but_last_n(resnet50, 2)\n",
    "        resnet50 = resnet50.to(device)\n",
    "        return resnet50\n",
    "\n",
    "    elif model_name == \"resnet101\":\n",
    "        resnet101 = models.resnet101(weights='DEFAULT')\n",
    "        resnet101.fc = nn.Linear(resnet101.fc.in_features, 7)\n",
    "        resnet101 = freeze_all_but_last_n(resnet101, 2)\n",
    "        resnet101 = resnet101.to(device)\n",
    "        return resnet101\n",
    "    \n",
    "    elif model_name == \"resnet152\":\n",
    "        resnet152 = models.resnet152(weights='DEFAULT')\n",
    "        resnet152.fc = nn.Linear(resnet152.fc.in_features, 7)\n",
    "        resnet152 = freeze_all_but_last_n(resnet152, 2)\n",
    "        resnet152 = resnet152.to(device)\n",
    "        return resnet152\n",
    "    \n",
    "    elif model_name == \"vgg16\":\n",
    "        vgg16 = models.vgg16(weights='DEFAULT')\n",
    "        vgg16.classifier[6] = nn.Linear(vgg16.classifier[6].in_features, 7)\n",
    "        vgg16 = freeze_all_but_last_n(vgg16, 1)\n",
    "        vgg16 = vgg16.to(device)\n",
    "        return vgg16\n",
    "    \n",
    "    elif model_name == \"alexnet\":\n",
    "        alexnet = models.alexnet(weights='DEFAULT')\n",
    "        alexnet.classifier[6] = nn.Linear(alexnet.classifier[6].in_features, 7)\n",
    "        alexnet = freeze_all_but_last_n(alexnet, 1)\n",
    "        alexnet = alexnet.to(device)\n",
    "        return alexnet\n",
    "    \n",
    "    elif model_name == \"googlenet\":\n",
    "        googlenet = models.googlenet(weights='DEFAULT')\n",
    "        googlenet.fc = nn.Linear(googlenet.fc.in_features, 7)\n",
    "        googlenet = freeze_all_but_last_n(googlenet, 2)\n",
    "        googlenet = googlenet.to(device)\n",
    "        return googlenet\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Model {model_name} not recognized. Please choose a valid model name.\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb37080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(config, transform=None):\n",
    "    dataset_type = config[\"dataset_class\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    seed = 42 \n",
    "\n",
    "    if dataset_type == \"ImageClass1\": # using train and test directories\n",
    "        train_dataset = ImageDataset1(root_dir=train_dir, transform=transform, classes=classes, is_train=True)\n",
    "        val_dataset = ImageDataset1(root_dir=test_dir, transform=transform, classes=classes, is_train=False)\n",
    "    else: # Splitting train into train and validation sets\n",
    "        train_dataset = ImageDataset2(\n",
    "            root_dir=train_dir,\n",
    "            classes=classes,\n",
    "            transform=transform,\n",
    "            is_train=True,\n",
    "            split_ratio=0.8,\n",
    "            seed=seed\n",
    "        )\n",
    "        val_dataset = ImageDataset2(\n",
    "            root_dir=train_dir,\n",
    "            classes=classes,\n",
    "            transform=transform,\n",
    "            is_train=False,\n",
    "            split_ratio=0.8,\n",
    "            seed=seed\n",
    "        )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def validate_model(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    return val_loss / len(val_loader), 100. * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e18a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model, method):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "            if method == \"xavier\":\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "            elif method == \"kaiming\":\n",
    "                nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "\n",
    "def should_initialize(model_type):\n",
    "    return model_type == \"scratch\"  # only initialize scratch models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa3b5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def train_model(config):\n",
    "    model_type , model_name = config[\"model_choice\"]\n",
    "    train_loader, val_loader = get_dataloaders(config)\n",
    "\n",
    "    init_method = config[\"init_method\"]\n",
    "\n",
    "    model = load_model(model_name)\n",
    "    model.to(device)\n",
    "    time_stamp = datetime.now().strftime(\"%Y%m%d_%H\")\n",
    "    unique_config = f\"{model_name}_{config['dataset_class']}_{config['optimizer']}_{config['init_method']}_{config['batch_size']}_{config['lr']}_time_{time_stamp}\"\n",
    "\n",
    "    if should_initialize(config[\"model_choice\"][0]) and init_method != \"default\":\n",
    "        initialize_weights(model, init_method)\n",
    "\n",
    "    # Optimizer\n",
    "    if config[\"optimizer\"] == \"adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "    elif config[\"optimizer\"] == \"sgd\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=config[\"lr\"])\n",
    "    else:\n",
    "        optimizer = torch.optim.RMSprop(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "\n",
    "    epochs = config[\"epochs\"]\n",
    "    save_interval = 2 if model_type == \"pretrained\" else 50\n",
    "    save_dir = os.path.join(\"logs_224_pretrained\", \"checkpoints\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    total_batches = len(train_loader)\n",
    "    total_steps = epochs * total_batches\n",
    "    progress_bar = tqdm(total=total_steps, dynamic_ncols=True, desc=\"Training\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            # Update tqdm\n",
    "            train_loss = running_loss / (i + 1)\n",
    "            train_acc = 100. * correct / total\n",
    "            progress_bar.update(1)\n",
    "            progress_bar.set_postfix({\n",
    "                \"Epoch\": f\"{epoch+1}/{epochs}\",\n",
    "                \"Train Loss\": f\"{train_loss:.4f}\",\n",
    "                \"Train Acc\": f\"{train_acc:.2f}%\"\n",
    "            })\n",
    "\n",
    "        # Validation phase\n",
    "        val_loss, val_acc = validate_model(model, val_loader, criterion)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "        # Save model checkpoint\n",
    "        if (epoch + 1) % save_interval == 0:\n",
    "            os.makedirs(os.path.join(save_dir, unique_config), exist_ok=True)\n",
    "            torch.save(model.state_dict(), os.path.join(save_dir, unique_config, f\"epoch_{epoch+1}.pt\"))\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= 20:\n",
    "                progress_bar.set_description(\"Early Stopping\")\n",
    "                break\n",
    "    \n",
    "        progress_bar.set_postfix({\"Epoch\": f\"{epoch+1}/{epochs}\", \"Train Loss\": f\"{train_loss:.4f}\", \"Train Acc\": f\"{train_acc:.2f}%\", \"Val Loss\": f\"{val_loss:.4f}\", \"Val Acc\": f\"{val_acc:.2f}%\"})\n",
    "    progress_bar.close()\n",
    "\n",
    "    # Save metrics\n",
    "    os.makedirs(os.path.join(save_dir, unique_config), exist_ok=True)\n",
    "    torch.save({\n",
    "        \"train_losses\": train_losses,\n",
    "        \"val_losses\": val_losses,\n",
    "        \"train_accs\": train_accs,\n",
    "        \"val_accs\": val_accs\n",
    "    }, os.path.join(save_dir, unique_config, \"metrics.pt\"))\n",
    "\n",
    "\n",
    "    torch.save(model.state_dict(), os.path.join(save_dir, unique_config, \"final_model.pt\"))\n",
    "\n",
    "    return max(val_accs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a552ec35",
   "metadata": {},
   "source": [
    "## Manual Configurations Pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6759c454",
   "metadata": {},
   "outputs": [],
   "source": [
    "config1 = {\n",
    "    \"model_choice\": (\"pretrained\", \"resnet18\"),\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"lr\": 0.01,\n",
    "    \"batch_size\": 32,\n",
    "    \"dataset_class\": \"ImageClass2\",\n",
    "    \"epochs\": 3,\n",
    "    \"init_method\": \"default\",\n",
    "}\n",
    "\n",
    "config2 = {\n",
    "    \"model_choice\": (\"pretrained\", \"resnet18\"),\n",
    "    \"optimizer\": \"sgd\",\n",
    "    \"lr\": 0.01,\n",
    "    \"batch_size\": 64,\n",
    "    \"dataset_class\": \"ImageClass2\",\n",
    "    \"epochs\": 3,\n",
    "    \"init_method\": \"default\",\n",
    "}\n",
    "\n",
    "config3 = {\n",
    "    \"model_choice\": (\"pretrained\", \"resnet18\"),\n",
    "    \"optimizer\": \"rmsprop\",\n",
    "    \"lr\": 0.01,\n",
    "    \"batch_size\": 16,\n",
    "    \"dataset_class\": \"ImageClass1\",\n",
    "    \"epochs\": 3,\n",
    "    \"init_method\": \"default\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1e6311",
   "metadata": {},
   "outputs": [],
   "source": [
    "config4 = {\n",
    "    \"model_choice\": (\"pretrained\", \"resnet34\"),\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"lr\": 0.01,\n",
    "    \"batch_size\": 32,\n",
    "    \"dataset_class\": \"ImageClass1\",\n",
    "    \"epochs\": 3,\n",
    "    \"init_method\": \"default\",\n",
    "}\n",
    "\n",
    "config5 = {\n",
    "    \"model_choice\": (\"pretrained\", \"resnet34\"),\n",
    "    \"optimizer\": \"sgd\",\n",
    "    \"lr\": 0.01,\n",
    "    \"batch_size\": 64,\n",
    "    \"dataset_class\": \"ImageClass2\",\n",
    "    \"epochs\": 3,\n",
    "    \"init_method\": \"default\",\n",
    "}\n",
    "\n",
    "config6 = {\n",
    "    \"model_choice\": (\"pretrained\", \"resnet34\"),\n",
    "    \"optimizer\": \"rmsprop\",\n",
    "    \"lr\": 0.01,\n",
    "    \"batch_size\": 32,\n",
    "    \"dataset_class\": \"ImageClass2\",\n",
    "    \"epochs\": 3,\n",
    "    \"init_method\": \"default\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f495b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config7 = {\n",
    "    \"model_choice\": (\"pretrained\", \"resnet50\"),\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"lr\": 0.01,\n",
    "    \"batch_size\": 32,\n",
    "    \"dataset_class\": \"ImageClass2\",\n",
    "    \"epochs\": 3,\n",
    "    \"init_method\": \"default\",\n",
    "}\n",
    "\n",
    "config8 = {\n",
    "    \"model_choice\": (\"pretrained\", \"resnet50\"),\n",
    "    \"optimizer\": \"sgd\",\n",
    "    \"lr\": 0.01,\n",
    "    \"batch_size\": 64,\n",
    "    \"dataset_class\": \"ImageClass1\",\n",
    "    \"epochs\": 3,\n",
    "    \"init_method\": \"default\",\n",
    "}\n",
    "\n",
    "config9 = {\n",
    "    \"model_choice\": (\"pretrained\", \"resnet50\"),\n",
    "    \"optimizer\": \"rmsprop\",\n",
    "    \"lr\": 0.001,\n",
    "    \"batch_size\": 16,\n",
    "    \"dataset_class\": \"ImageClass2\",\n",
    "    \"epochs\": 3,\n",
    "    \"init_method\": \"default\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3321e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config10 = {\n",
    "    \"model_choice\": (\"pretrained\", \"resnet101\"),\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"lr\": 0.01,\n",
    "    \"batch_size\": 32,\n",
    "    \"dataset_class\": \"ImageClass2\",\n",
    "    \"epochs\": 3,\n",
    "    \"init_method\": \"default\",\n",
    "}\n",
    "\n",
    "config11 = {\n",
    "    \"model_choice\": (\"pretrained\", \"resnet101\"),\n",
    "    \"optimizer\": \"sgd\",\n",
    "    \"lr\": 0.01,\n",
    "    \"batch_size\": 32,\n",
    "    \"dataset_class\": \"ImageClass1\",\n",
    "    \"epochs\": 3,\n",
    "    \"init_method\": \"default\",\n",
    "}\n",
    "\n",
    "config12 = {\n",
    "    \"model_choice\": (\"pretrained\", \"resnet101\"),\n",
    "    \"optimizer\": \"rmsprop\",\n",
    "    \"lr\": 0.01,\n",
    "    \"batch_size\": 64,\n",
    "    \"dataset_class\": \"ImageClass2\",\n",
    "    \"epochs\": 3,\n",
    "    \"init_method\": \"default\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e7c4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config13 = {\n",
    "    \"model_choice\": (\"pretrained\", \"resnet152\"),\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"lr\": 0.01,\n",
    "    \"batch_size\": 32,\n",
    "    \"dataset_class\": \"ImageClass2\",\n",
    "    \"epochs\": 3,\n",
    "    \"init_method\": \"default\",\n",
    "}\n",
    "\n",
    "config14 = {\n",
    "    \"model_choice\": (\"pretrained\", \"resnet152\"),\n",
    "    \"optimizer\": \"sgd\",\n",
    "    \"lr\": 0.01,\n",
    "    \"batch_size\": 64,\n",
    "    \"dataset_class\": \"ImageClass1\",\n",
    "    \"epochs\": 3,\n",
    "    \"init_method\": \"default\",\n",
    "}\n",
    "\n",
    "config15 = {\n",
    "    \"model_choice\": (\"pretrained\", \"resnet152\"),\n",
    "    \"optimizer\": \"rmsprop\",\n",
    "    \"lr\": 0.01,\n",
    "    \"batch_size\": 16,\n",
    "    \"dataset_class\": \"ImageClass2\",\n",
    "    \"epochs\": 3,\n",
    "    \"init_method\": \"default\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe6c98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config16 = {\n",
    "    \"model_choice\": (\"pretrained\", \"vgg16\"),\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"lr\": 0.01,\n",
    "    \"batch_size\": 32,\n",
    "    \"dataset_class\": \"ImageClass2\",\n",
    "    \"epochs\": 3,\n",
    "    \"init_method\": \"default\",\n",
    "}\n",
    "\n",
    "config17 = {\n",
    "    \"model_choice\": (\"pretrained\", \"vgg16\"),\n",
    "    \"optimizer\": \"sgd\",\n",
    "    \"lr\": 0.01,\n",
    "    \"batch_size\": 64,\n",
    "    \"dataset_class\": \"ImageClass1\",\n",
    "    \"epochs\": 3,\n",
    "    \"init_method\": \"default\",\n",
    "}\n",
    "\n",
    "config18 = {\n",
    "    \"model_choice\": (\"pretrained\", \"vgg16\"),\n",
    "    \"optimizer\": \"rmsprop\",\n",
    "    \"lr\": 0.01,\n",
    "    \"batch_size\": 32,\n",
    "    \"dataset_class\": \"ImageClass2\",\n",
    "    \"epochs\": 3,\n",
    "    \"init_method\": \"default\",\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ea4843",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config19 = {\n",
    "    \"model_choice\": (\"pretrained\", \"alexnet\"),\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"lr\": 0.01,\n",
    "    \"batch_size\": 32,\n",
    "    \"dataset_class\": \"ImageClass1\",\n",
    "    \"epochs\": 3,\n",
    "    \"init_method\": \"default\",\n",
    "}\n",
    "\n",
    "config20 = {\n",
    "    \"model_choice\": (\"pretrained\", \"alexnet\"),\n",
    "    \"optimizer\": \"sgd\",\n",
    "    \"lr\": 0.01,\n",
    "    \"batch_size\": 64,\n",
    "    \"dataset_class\": \"ImageClass2\",\n",
    "    \"epochs\": 3,\n",
    "    \"init_method\": \"default\",\n",
    "}\n",
    "\n",
    "config21 = {\n",
    "    \"model_choice\": (\"pretrained\", \"alexnet\"),\n",
    "    \"optimizer\": \"rmsprop\",\n",
    "    \"lr\": 0.01,\n",
    "    \"batch_size\": 16,\n",
    "    \"dataset_class\": \"ImageClass2\",\n",
    "    \"epochs\": 3,\n",
    "    \"init_method\": \"default\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063d74d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config22 = {\n",
    "    \"model_choice\": (\"pretrained\", \"googlenet\"),\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"lr\": 0.01,\n",
    "    \"batch_size\": 32,\n",
    "    \"dataset_class\": \"ImageClass2\",\n",
    "    \"epochs\": 3,\n",
    "    \"init_method\": \"default\",\n",
    "}\n",
    "\n",
    "config23 = {\n",
    "    \"model_choice\": (\"pretrained\", \"googlenet\"),\n",
    "    \"optimizer\": \"sgd\",\n",
    "    \"lr\": 0.01,\n",
    "    \"batch_size\": 64,\n",
    "    \"dataset_class\": \"ImageClass1\",\n",
    "    \"epochs\": 3,\n",
    "    \"init_method\": \"default\",\n",
    "}\n",
    "\n",
    "config24 = {\n",
    "    \"model_choice\": (\"pretrained\", \"googlenet\"),\n",
    "    \"optimizer\": \"rmsprop\",\n",
    "    \"lr\": 0.01,\n",
    "    \"batch_size\": 16,\n",
    "    \"dataset_class\": \"ImageClass2\",\n",
    "    \"epochs\": 3,\n",
    "    \"init_method\": \"default\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8794aee",
   "metadata": {},
   "source": [
    "## Nezar Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73b0439",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1236/1236 [10:45<00:00,  1.92it/s, Epoch=12/12, Train Loss=0.7375, Train Acc=74.68%]\n",
      "Training: 100%|██████████| 2060/2060 [09:40<00:00,  3.55it/s, Epoch=10/10, Train Loss=0.7167, Train Acc=74.61%] \n",
      "Training: 100%|██████████| 5150/5150 [13:50<00:00,  6.20it/s, Epoch=10/10, Train Loss=0.7702, Train Acc=72.49%] \n",
      "Training: 100%|██████████| 2580/2580 [12:27<00:00,  3.45it/s, Epoch=10/10, Train Loss=0.6474, Train Acc=77.51%] \n",
      "Training: 100%|██████████| 1545/1545 [13:55<00:00,  1.85it/s, Epoch=15/15, Train Loss=0.6698, Train Acc=76.61%]\n",
      "Training: 100%|██████████| 2060/2060 [10:03<00:00,  3.41it/s, Epoch=10/10, Train Loss=0.6546, Train Acc=76.71%] \n",
      "Training: 100%|██████████| 2060/2060 [11:03<00:00,  3.11it/s, Epoch=10/10, Train Loss=0.4395, Train Acc=85.64%] \n",
      "Training: 100%|██████████| 1935/1935 [16:25<00:00,  1.96it/s, Epoch=15/15, Train Loss=0.6002, Train Acc=82.03%] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "78.74513618677042"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_model(config2)\n",
    "train_model(config1)\n",
    "train_model(config3)\n",
    "train_model(config4)\n",
    "train_model(config5)\n",
    "train_model(config6)\n",
    "train_model(config7)\n",
    "train_model(config8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6809b8b3",
   "metadata": {},
   "source": [
    "## Kiro Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a51001",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4944/4944 [12:59<00:00,  6.34it/s, Epoch=12/12, Train Loss=0.4669, Train Acc=83.60%, Val Loss=0.5681, Val Acc=82.44%] \n",
      "Training: 100%|██████████| 2060/2060 [10:10<00:00,  3.37it/s, Epoch=10/10, Train Loss=0.5542, Train Acc=83.39%, Val Loss=0.6789, Val Acc=79.34%] \n",
      "Training:  70%|███████   | 2174/3096 [10:53<04:34,  3.36it/s, Epoch=9/12, Train Loss=0.5993, Train Acc=80.65%]                                   "
     ]
    }
   ],
   "source": [
    "train_model(config9)\n",
    "train_model(config10)\n",
    "train_model(config11)\n",
    "train_model(config12)\n",
    "train_model(config13)\n",
    "train_model(config14)\n",
    "train_model(config15)\n",
    "train_model(config16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a97656d",
   "metadata": {},
   "source": [
    "## Brocoli Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f704682d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(config17)\n",
    "train_model(config18)\n",
    "train_model(config19)\n",
    "train_model(config20)\n",
    "train_model(config21)\n",
    "train_model(config22)\n",
    "train_model(config23)\n",
    "train_model(config24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8eb9a2",
   "metadata": {},
   "source": [
    "## Manual Configurations Simple CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2d7b8c",
   "metadata": {},
   "source": [
    "# Functions to load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644f6a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_weights(model, path):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    return model\n",
    "\n",
    "def load_metrics(path):\n",
    "    return torch.load(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
