{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7a9cdca",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "facacd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kagglehub\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "import random\n",
    "import optuna\n",
    "from optuna.trial import TrialState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27e1b7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = kagglehub.dataset_download(\"sidharkal/sports-image-classification\")\n",
    "\n",
    "# print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f55cdca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data\"\n",
    "images_dir = \"../data/dataset/\"\n",
    "train_dir = images_dir + \"train/\"\n",
    "\n",
    "badminton_train_dir = train_dir + \"Badminton/\"\n",
    "tennis_train_dir = train_dir + \"Tennis/\"\n",
    "cricket_train_dir = train_dir + \"Cricket/\"\n",
    "soccer_train_dir = train_dir + \"Soccer/\"\n",
    "swimming_train_dir = train_dir + \"Swimming/\"\n",
    "karate_train_dir = train_dir + \"Karate/\"\n",
    "wrestling_train_dir = train_dir + \"Wrestling/\"\n",
    "\n",
    "test_dir = images_dir + \"test/\"\n",
    "\n",
    "badminton_test_dir = test_dir + \"Badminton/\"\n",
    "tennis_test_dir = test_dir + \"Tennis/\"\n",
    "cricket_test_dir = test_dir + \"Cricket/\"\n",
    "soccer_test_dir = test_dir + \"Soccer/\"\n",
    "swimming_test_dir = test_dir + \"Swimming/\"\n",
    "karate_test_dir = test_dir + \"Karate/\"\n",
    "wrestling_test_dir = test_dir + \"Wrestling/\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f604dad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1cfbd13e4b0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72c18c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shutil.copytree(path, data_dir, dirs_exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7865e3c5",
   "metadata": {},
   "source": [
    "## Dataset class and data manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d9059c",
   "metadata": {},
   "source": [
    "### Dataset class 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fdeb8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['Badminton', 'Cricket', 'Tennis', 'Swimming', 'Soccer', 'Wrestling', 'Karate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cac05d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset1(Dataset):\n",
    "    def __init__(self, root_dir, classes, transform=None, is_train=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Directory with all the class folders\n",
    "            classes (list): List of class names (subfolder names)\n",
    "            transform (callable, optional): Optional transform to be applied on a sample\n",
    "            is_train (bool): Whether this is training data or not\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.classes = classes\n",
    "        self.transform = transform\n",
    "        self.is_train = is_train\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n",
    "        self.samples = []\n",
    "\n",
    "        # Default transforms if none provided\n",
    "        if self.transform is None:\n",
    "            if is_train:\n",
    "                self.transform = T.Compose([\n",
    "                    T.Resize(224),\n",
    "                    T.CenterCrop(224), # Resize to 128x128\n",
    "                    T.ToTensor(),\n",
    "                ])\n",
    "            else:\n",
    "                self.transform = T.Compose([\n",
    "                    T.Resize(224),\n",
    "                    T.CenterCrop(224), # Resize to 128x128\n",
    "                    T.ToTensor(),\n",
    "                ])\n",
    "\n",
    "        for idx, cls in enumerate(classes):\n",
    "            class_folder = os.path.join(root_dir, cls)\n",
    "            if not os.path.isdir(class_folder):\n",
    "                continue\n",
    "            for img_name in os.listdir(class_folder):\n",
    "                if img_name.lower().endswith(('jpg', 'jpeg', 'png')):\n",
    "                    img_path = os.path.join(class_folder, img_name)\n",
    "                    self.samples.append((img_path, idx))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx, retry=0):\n",
    "        img_path, label = self.samples[idx]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, label\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {str(e)}\")\n",
    "            if retry < 3:\n",
    "                return self.__getitem__(random.randint(0, len(self)-1), retry=retry+1)\n",
    "            else:\n",
    "                raise RuntimeError(\"Too many failed image loads.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d18aa6",
   "metadata": {},
   "source": [
    "### Dataset class 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d127a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset2(Dataset):\n",
    "    def __init__(self, root_dir, classes, transform=None, is_train=True, split_ratio=0.8, seed=42):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Directory with all the class folders\n",
    "            classes (list): List of class names (subfolder names)\n",
    "            transform (callable, optional): Optional transform to be applied on a sample\n",
    "            is_train (bool): Whether this is training data or not\n",
    "            split_ratio (float): Ratio for training data (default is 0.8)\n",
    "            seed (int): Seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.classes = classes\n",
    "        self.transform = transform\n",
    "        self.is_train = is_train\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n",
    "        self.samples = []\n",
    "\n",
    "        all_samples = []\n",
    "        for idx, cls in enumerate(classes):\n",
    "            class_folder = os.path.join(root_dir, cls)\n",
    "            if not os.path.isdir(class_folder):\n",
    "                continue\n",
    "            for img_name in os.listdir(class_folder):\n",
    "                if img_name.lower().endswith(('jpg', 'jpeg', 'png')):\n",
    "                    img_path = os.path.join(class_folder, img_name)\n",
    "                    all_samples.append((img_path, idx))\n",
    "\n",
    "        # Shuffle and split once\n",
    "        random.seed(seed)\n",
    "        random.shuffle(all_samples)\n",
    "        split_point = int(len(all_samples) * split_ratio)\n",
    "        if is_train:\n",
    "            self.samples = all_samples[:split_point]\n",
    "        else:\n",
    "            self.samples = all_samples[split_point:]\n",
    "\n",
    "        # Set default transforms if not provided\n",
    "        if self.transform is None:\n",
    "            if is_train:\n",
    "                self.transform = T.Compose([\n",
    "                    T.Resize(224),\n",
    "                    T.CenterCrop(224), # Resize to 128x128\n",
    "                    T.ToTensor(),\n",
    "                ])\n",
    "            else:\n",
    "                self.transform = T.Compose([\n",
    "                    T.Resize(224),\n",
    "                    T.CenterCrop(224),\n",
    "                    T.ToTensor(),\n",
    "                ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx, retry=0):\n",
    "        img_path, label = self.samples[idx]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, label\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {str(e)}\")\n",
    "            if retry < 3:\n",
    "                return self.__getitem__(random.randint(0, len(self)-1), retry=retry+1)\n",
    "            else:\n",
    "                raise RuntimeError(\"Too many failed image loads.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa98d025",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['Badminton', 'Cricket', 'Tennis', 'Swimming', 'Soccer', 'Wrestling', 'Karate']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e36523",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6902ccd0",
   "metadata": {},
   "source": [
    "# Utils for Pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db259a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_all_but_last_n(model, n=2):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Get all modules with parameters\n",
    "    modules_with_params = [m for m in model.modules() if any(p.requires_grad is False for p in m.parameters())]\n",
    "\n",
    "    # Unfreeze last n modules with parameters\n",
    "    for module in modules_with_params[-n:]:\n",
    "        for param in module.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def print_trainable_params(model):\n",
    "    print(\"Trainable Parameters:\")\n",
    "    total = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            num_params = param.numel()\n",
    "            # print(f\"{name}: {num_params}\")\n",
    "            total += num_params\n",
    "    print(f\"Total Trainable Parameters: {total}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c246224",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d5409bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "    if model_name == \"resnet18\":\n",
    "        resnet18 = models.resnet18(weights='DEFAULT')\n",
    "        resnet18.fc = nn.Linear(resnet18.fc.in_features, 7)  # Change the output layer to match the number of classes\n",
    "        resnet18 = freeze_all_but_last_n(resnet18, 2)  # Freeze all but the last 2 layers\n",
    "        resnet18 = resnet18.to(device)\n",
    "        return resnet18\n",
    "\n",
    "    elif model_name == \"resnet34\":\n",
    "        resnet34 = models.resnet34(weights='DEFAULT')\n",
    "        resnet34.fc = nn.Linear(resnet34.fc.in_features, 7)\n",
    "        resnet34 = freeze_all_but_last_n(resnet34, 2)\n",
    "        resnet34 = resnet34.to(device)\n",
    "        return resnet34\n",
    "    \n",
    "    elif model_name == \"resnet50\":\n",
    "        resnet50 = models.resnet50(weights='DEFAULT')\n",
    "        resnet50.fc = nn.Linear(resnet50.fc.in_features, 7)\n",
    "        resnet50 = freeze_all_but_last_n(resnet50, 2)\n",
    "        resnet50 = resnet50.to(device)\n",
    "        return resnet50\n",
    "\n",
    "    elif model_name == \"resnet101\":\n",
    "        resnet101 = models.resnet101(weights='DEFAULT')\n",
    "        resnet101.fc = nn.Linear(resnet101.fc.in_features, 7)\n",
    "        resnet101 = freeze_all_but_last_n(resnet101, 2)\n",
    "        resnet101 = resnet101.to(device)\n",
    "        return resnet101\n",
    "    \n",
    "    elif model_name == \"resnet152\":\n",
    "        resnet152 = models.resnet152(weights='DEFAULT')\n",
    "        resnet152.fc = nn.Linear(resnet152.fc.in_features, 7)\n",
    "        resnet152 = freeze_all_but_last_n(resnet152, 2)\n",
    "        resnet152 = resnet152.to(device)\n",
    "        return resnet152\n",
    "    \n",
    "    elif model_name == \"vgg16\":\n",
    "        vgg16 = models.vgg16(weights='DEFAULT')\n",
    "        vgg16.classifier[6] = nn.Linear(vgg16.classifier[6].in_features, 7)\n",
    "        vgg16 = freeze_all_but_last_n(vgg16, 1)\n",
    "        vgg16 = vgg16.to(device)\n",
    "        return vgg16\n",
    "    \n",
    "    elif model_name == \"alexnet\":\n",
    "        alexnet = models.alexnet(weights='DEFAULT')\n",
    "        alexnet.classifier[6] = nn.Linear(alexnet.classifier[6].in_features, 7)\n",
    "        alexnet = freeze_all_but_last_n(alexnet, 1)\n",
    "        alexnet = alexnet.to(device)\n",
    "        return alexnet\n",
    "    \n",
    "    elif model_name == \"googlenet\":\n",
    "        googlenet = models.googlenet(weights='DEFAULT')\n",
    "        googlenet.fc = nn.Linear(googlenet.fc.in_features, 7)\n",
    "        googlenet = freeze_all_but_last_n(googlenet, 2)\n",
    "        googlenet = googlenet.to(device)\n",
    "        return googlenet\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Model {model_name} not recognized. Please choose a valid model name.\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18c3f8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(config, transform=None):\n",
    "    dataset_type = config[\"dataset_class\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    seed = 42 \n",
    "\n",
    "    if dataset_type == \"ImageClass1\": # using train and test directories\n",
    "        train_dataset = ImageDataset1(root_dir=train_dir, transform=transform, classes=classes, is_train=True)\n",
    "        val_dataset = ImageDataset1(root_dir=test_dir, transform=transform, classes=classes, is_train=False)\n",
    "    else: # Splitting train into train and validation sets\n",
    "        train_dataset = ImageDataset2(\n",
    "            root_dir=train_dir,\n",
    "            classes=classes,\n",
    "            transform=transform,\n",
    "            is_train=True,\n",
    "            split_ratio=0.8,\n",
    "            seed=seed\n",
    "        )\n",
    "        val_dataset = ImageDataset2(\n",
    "            root_dir=train_dir,\n",
    "            classes=classes,\n",
    "            transform=transform,\n",
    "            is_train=False,\n",
    "            split_ratio=0.8,\n",
    "            seed=seed\n",
    "        )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def validate_model(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    return val_loss / len(val_loader), 100. * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "336f0ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model, method):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "            if method == \"xavier\":\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "            elif method == \"kaiming\":\n",
    "                nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "\n",
    "def should_initialize(model_type):\n",
    "    return model_type == \"scratch\"  # only initialize scratch models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb3c8555",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def train_model(config):\n",
    "    model_type , model_name = config[\"model_choice\"]\n",
    "    train_loader, val_loader = get_dataloaders(config)\n",
    "\n",
    "    init_method = config[\"init_method\"]\n",
    "\n",
    "    model = load_model(model_name)\n",
    "    model.to(device)\n",
    "    time_stamp = datetime.now().strftime(\"%Y%m%d_%H\")\n",
    "    unique_config = f\"{model_name}_{config['dataset_class']}_{config['optimizer']}_{config['init_method']}_{config['batch_size']}_{config['lr']}_time_{time_stamp}\"\n",
    "\n",
    "    if should_initialize(config[\"model_choice\"][0]) and init_method != \"default\":\n",
    "        initialize_weights(model, init_method)\n",
    "\n",
    "    # Optimizer\n",
    "    if config[\"optimizer\"] == \"adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "    elif config[\"optimizer\"] == \"sgd\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=config[\"lr\"])\n",
    "    else:\n",
    "        optimizer = torch.optim.RMSprop(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "\n",
    "    epochs = config[\"epochs\"]\n",
    "    save_interval = 2 if model_type == \"pretrained\" else 50\n",
    "    save_dir = os.path.join(\"logs_224_pretrained_center_crop\", \"checkpoints\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    total_batches = len(train_loader)\n",
    "    total_steps = epochs * total_batches\n",
    "    progress_bar = tqdm(total=total_steps, dynamic_ncols=True, desc=\"Training\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            # Update tqdm\n",
    "            train_loss = running_loss / (i + 1)\n",
    "            train_acc = 100. * correct / total\n",
    "            progress_bar.update(1)\n",
    "            progress_bar.set_postfix({\n",
    "                \"Epoch\": f\"{epoch+1}/{epochs}\",\n",
    "                \"Train Loss\": f\"{train_loss:.4f}\",\n",
    "                \"Train Acc\": f\"{train_acc:.2f}%\"\n",
    "            })\n",
    "\n",
    "        # Validation phase\n",
    "        val_loss, val_acc = validate_model(model, val_loader, criterion)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "        # Save model checkpoint\n",
    "        if (epoch + 1) % save_interval == 0:\n",
    "            os.makedirs(os.path.join(save_dir, unique_config), exist_ok=True)\n",
    "            torch.save(model.state_dict(), os.path.join(save_dir, unique_config, f\"epoch_{epoch+1}.pt\"))\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= 20:\n",
    "                progress_bar.set_description(\"Early Stopping\")\n",
    "                break\n",
    "    \n",
    "        progress_bar.set_postfix({\"Epoch\": f\"{epoch+1}/{epochs}\", \"Train Loss\": f\"{train_loss:.4f}\", \"Train Acc\": f\"{train_acc:.2f}%\", \"Val Loss\": f\"{val_loss:.4f}\", \"Val Acc\": f\"{val_acc:.2f}%\"})\n",
    "    progress_bar.close()\n",
    "\n",
    "    # Save metrics\n",
    "    os.makedirs(os.path.join(save_dir, unique_config), exist_ok=True)\n",
    "    torch.save({\n",
    "        \"train_losses\": train_losses,\n",
    "        \"val_losses\": val_losses,\n",
    "        \"train_accs\": train_accs,\n",
    "        \"val_accs\": val_accs\n",
    "    }, os.path.join(save_dir, unique_config, \"metrics.pt\"))\n",
    "\n",
    "\n",
    "    torch.save(model.state_dict(), os.path.join(save_dir, unique_config, \"final_model.pt\"))\n",
    "\n",
    "    return max(val_accs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ab6728",
   "metadata": {},
   "source": [
    "## Manual Configurations Pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed46cc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config1 = {\n",
    "    \"model_choice\": (\"pretrained\", \"resnet18\"),\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"lr\": 0.01,\n",
    "    \"batch_size\": 32,\n",
    "    \"dataset_class\": \"ImageClass2\",\n",
    "    \"epochs\": 7,\n",
    "    \"init_method\": \"default\",\n",
    "}\n",
    "\n",
    "config2 = {\n",
    "    \"model_choice\": (\"pretrained\", \"resnet18\"),\n",
    "    \"optimizer\": \"sgd\",\n",
    "    \"lr\": 0.01,\n",
    "    \"batch_size\": 64,\n",
    "    \"dataset_class\": \"ImageClass2\",\n",
    "    \"epochs\": 7,\n",
    "    \"init_method\": \"default\",\n",
    "}\n",
    "\n",
    "config3 = {\n",
    "    \"model_choice\": (\"pretrained\", \"resnet18\"),\n",
    "    \"optimizer\": \"rmsprop\",\n",
    "    \"lr\": 0.01,\n",
    "    \"batch_size\": 16,\n",
    "    \"dataset_class\": \"ImageClass1\",\n",
    "    \"epochs\": 7,\n",
    "    \"init_method\": \"default\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77b8d44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config4 = {\n",
    "    \"model_choice\": (\"pretrained\", \"resnet34\"),\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"lr\": 0.01,\n",
    "    \"batch_size\": 32,\n",
    "    \"dataset_class\": \"ImageClass1\",\n",
    "    \"epochs\": 7,\n",
    "    \"init_method\": \"default\",\n",
    "}\n",
    "\n",
    "config5 = {\n",
    "    \"model_choice\": (\"pretrained\", \"resnet34\"),\n",
    "    \"optimizer\": \"sgd\",\n",
    "    \"lr\": 0.01,\n",
    "    \"batch_size\": 64,\n",
    "    \"dataset_class\": \"ImageClass2\",\n",
    "    \"epochs\": 7,\n",
    "    \"init_method\": \"default\",\n",
    "}\n",
    "\n",
    "config6 = {\n",
    "    \"model_choice\": (\"pretrained\", \"resnet34\"),\n",
    "    \"optimizer\": \"rmsprop\",\n",
    "    \"lr\": 0.01,\n",
    "    \"batch_size\": 32,\n",
    "    \"dataset_class\": \"ImageClass2\",\n",
    "    \"epochs\": 7,\n",
    "    \"init_method\": \"default\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb79b84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config7 = {\n",
    "    \"model_choice\": (\"pretrained\", \"resnet50\"),\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"lr\": 0.01,\n",
    "    \"batch_size\": 32,\n",
    "    \"dataset_class\": \"ImageClass2\",\n",
    "    \"epochs\": 7,\n",
    "    \"init_method\": \"default\",\n",
    "}\n",
    "\n",
    "config8 = {\n",
    "    \"model_choice\": (\"pretrained\", \"resnet50\"),\n",
    "    \"optimizer\": \"sgd\",\n",
    "    \"lr\": 0.01,\n",
    "    \"batch_size\": 64,\n",
    "    \"dataset_class\": \"ImageClass1\",\n",
    "    \"epochs\": 7,\n",
    "    \"init_method\": \"default\",\n",
    "}\n",
    "\n",
    "config9 = {\n",
    "    \"model_choice\": (\"pretrained\", \"resnet50\"),\n",
    "    \"optimizer\": \"rmsprop\",\n",
    "    \"lr\": 0.001,\n",
    "    \"batch_size\": 16,\n",
    "    \"dataset_class\": \"ImageClass2\",\n",
    "    \"epochs\": 7,\n",
    "    \"init_method\": \"default\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9cbd7164",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config10 = {\n",
    "    \"model_choice\": (\"pretrained\", \"resnet101\"),\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"lr\": 0.01,\n",
    "    \"batch_size\": 32,\n",
    "    \"dataset_class\": \"ImageClass2\",\n",
    "    \"epochs\": 7,\n",
    "    \"init_method\": \"default\",\n",
    "}\n",
    "\n",
    "config11 = {\n",
    "    \"model_choice\": (\"pretrained\", \"resnet101\"),\n",
    "    \"optimizer\": \"sgd\",\n",
    "    \"lr\": 0.01,\n",
    "    \"batch_size\": 32,\n",
    "    \"dataset_class\": \"ImageClass1\",\n",
    "    \"epochs\": 7,\n",
    "    \"init_method\": \"default\",\n",
    "}\n",
    "\n",
    "config12 = {\n",
    "    \"model_choice\": (\"pretrained\", \"resnet101\"),\n",
    "    \"optimizer\": \"rmsprop\",\n",
    "    \"lr\": 0.01,\n",
    "    \"batch_size\": 64,\n",
    "    \"dataset_class\": \"ImageClass2\",\n",
    "    \"epochs\": 7,\n",
    "    \"init_method\": \"default\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7caecbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config13 = {\n",
    "    \"model_choice\": (\"pretrained\", \"resnet152\"),\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"lr\": 0.01,\n",
    "    \"batch_size\": 32,\n",
    "    \"dataset_class\": \"ImageClass2\",\n",
    "    \"epochs\": 7,\n",
    "    \"init_method\": \"default\",\n",
    "}\n",
    "\n",
    "config14 = {\n",
    "    \"model_choice\": (\"pretrained\", \"resnet152\"),\n",
    "    \"optimizer\": \"sgd\",\n",
    "    \"lr\": 0.01,\n",
    "    \"batch_size\": 64,\n",
    "    \"dataset_class\": \"ImageClass1\",\n",
    "    \"epochs\": 7,\n",
    "    \"init_method\": \"default\",\n",
    "}\n",
    "\n",
    "config15 = {\n",
    "    \"model_choice\": (\"pretrained\", \"resnet152\"),\n",
    "    \"optimizer\": \"rmsprop\",\n",
    "    \"lr\": 0.01,\n",
    "    \"batch_size\": 16,\n",
    "    \"dataset_class\": \"ImageClass2\",\n",
    "    \"epochs\": 7,\n",
    "    \"init_method\": \"default\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48faf4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config16 = {\n",
    "    \"model_choice\": (\"pretrained\", \"vgg16\"),\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"lr\": 0.01,\n",
    "    \"batch_size\": 32,\n",
    "    \"dataset_class\": \"ImageClass2\",\n",
    "    \"epochs\": 7,\n",
    "    \"init_method\": \"default\",\n",
    "}\n",
    "\n",
    "config17 = {\n",
    "    \"model_choice\": (\"pretrained\", \"vgg16\"),\n",
    "    \"optimizer\": \"sgd\",\n",
    "    \"lr\": 0.01,\n",
    "    \"batch_size\": 64,\n",
    "    \"dataset_class\": \"ImageClass1\",\n",
    "    \"epochs\": 7,\n",
    "    \"init_method\": \"default\",\n",
    "}\n",
    "\n",
    "config18 = {\n",
    "    \"model_choice\": (\"pretrained\", \"vgg16\"),\n",
    "    \"optimizer\": \"rmsprop\",\n",
    "    \"lr\": 0.01,\n",
    "    \"batch_size\": 32,\n",
    "    \"dataset_class\": \"ImageClass2\",\n",
    "    \"epochs\": 7,\n",
    "    \"init_method\": \"default\",\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c83153c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config19 = {\n",
    "    \"model_choice\": (\"pretrained\", \"alexnet\"),\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"lr\": 0.01,\n",
    "    \"batch_size\": 32,\n",
    "    \"dataset_class\": \"ImageClass1\",\n",
    "    \"epochs\": 7,\n",
    "    \"init_method\": \"default\",\n",
    "}\n",
    "\n",
    "config20 = {\n",
    "    \"model_choice\": (\"pretrained\", \"alexnet\"),\n",
    "    \"optimizer\": \"sgd\",\n",
    "    \"lr\": 0.01,\n",
    "    \"batch_size\": 64,\n",
    "    \"dataset_class\": \"ImageClass2\",\n",
    "    \"epochs\": 7,\n",
    "    \"init_method\": \"default\",\n",
    "}\n",
    "\n",
    "config21 = {\n",
    "    \"model_choice\": (\"pretrained\", \"alexnet\"),\n",
    "    \"optimizer\": \"rmsprop\",\n",
    "    \"lr\": 0.01,\n",
    "    \"batch_size\": 16,\n",
    "    \"dataset_class\": \"ImageClass2\",\n",
    "    \"epochs\": 7,\n",
    "    \"init_method\": \"default\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14707594",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config22 = {\n",
    "    \"model_choice\": (\"pretrained\", \"googlenet\"),\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"lr\": 0.01,\n",
    "    \"batch_size\": 32,\n",
    "    \"dataset_class\": \"ImageClass2\",\n",
    "    \"epochs\": 7,\n",
    "    \"init_method\": \"default\",\n",
    "}\n",
    "\n",
    "config23 = {\n",
    "    \"model_choice\": (\"pretrained\", \"googlenet\"),\n",
    "    \"optimizer\": \"sgd\",\n",
    "    \"lr\": 0.01,\n",
    "    \"batch_size\": 64,\n",
    "    \"dataset_class\": \"ImageClass1\",\n",
    "    \"epochs\": 7,\n",
    "    \"init_method\": \"default\",\n",
    "}\n",
    "\n",
    "config24 = {\n",
    "    \"model_choice\": (\"pretrained\", \"googlenet\"),\n",
    "    \"optimizer\": \"rmsprop\",\n",
    "    \"lr\": 0.01,\n",
    "    \"batch_size\": 16,\n",
    "    \"dataset_class\": \"ImageClass2\",\n",
    "    \"epochs\": 7,\n",
    "    \"init_method\": \"default\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b94668",
   "metadata": {},
   "source": [
    "## Nezar Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2dfb7616",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 721/721 [07:35<00:00,  1.58it/s, Epoch=7/7, Train Loss=0.4512, Train Acc=86.07%, Val Loss=0.4372, Val Acc=87.55%]\n",
      "Training: 100%|██████████| 1442/1442 [08:14<00:00,  2.92it/s, Epoch=7/7, Train Loss=0.2371, Train Acc=91.61%, Val Loss=0.3466, Val Acc=89.79%] \n",
      "Training: 100%|██████████| 3605/3605 [11:12<00:00,  5.36it/s, Epoch=7/7, Train Loss=0.3111, Train Acc=88.77%, Val Loss=0.4130, Val Acc=86.87%]  \n",
      "Training: 100%|██████████| 1806/1806 [10:13<00:00,  2.94it/s, Epoch=7/7, Train Loss=0.2032, Train Acc=93.00%, Val Loss=0.4095, Val Acc=87.79%] \n",
      "Training: 100%|██████████| 721/721 [08:43<00:00,  1.38it/s, Epoch=7/7, Train Loss=0.4063, Train Acc=87.30%, Val Loss=0.3933, Val Acc=87.55%]  \n",
      "Training: 100%|██████████| 1442/1442 [08:53<00:00,  2.70it/s, Epoch=7/7, Train Loss=0.1750, Train Acc=93.85%, Val Loss=0.2756, Val Acc=91.37%] \n",
      "Training: 100%|██████████| 1442/1442 [10:10<00:00,  2.36it/s, Epoch=7/7, Train Loss=0.0297, Train Acc=99.30%, Val Loss=0.1687, Val Acc=95.02%] \n",
      "Training: 100%|██████████| 903/903 [12:44<00:00,  1.18it/s, Epoch=7/7, Train Loss=0.5154, Train Acc=88.49%, Val Loss=0.5356, Val Acc=87.74%]  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "87.7431906614786"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(config2)\n",
    "train_model(config1)\n",
    "train_model(config3)\n",
    "train_model(config4)\n",
    "train_model(config5)\n",
    "train_model(config6)\n",
    "train_model(config7)\n",
    "train_model(config8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7908a84e",
   "metadata": {},
   "source": [
    "## Kiro Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d20344e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2884/2884 [10:24<00:00,  4.62it/s, Epoch=7/7, Train Loss=0.1356, Train Acc=96.23%, Val Loss=0.1775, Val Acc=94.05%]  \n",
      "Training: 100%|██████████| 1442/1442 [12:15<00:00,  1.96it/s, Epoch=7/7, Train Loss=0.0746, Train Acc=97.23%, Val Loss=0.2437, Val Acc=93.13%] \n",
      "Training: 100%|██████████| 1806/1806 [15:10<00:00,  1.98it/s, Epoch=7/7, Train Loss=0.3554, Train Acc=90.74%, Val Loss=0.3861, Val Acc=89.30%]  \n",
      "Training: 100%|██████████| 721/721 [11:52<00:00,  1.01it/s, Epoch=7/7, Train Loss=0.0283, Train Acc=99.19%, Val Loss=0.2048, Val Acc=93.80%]  \n",
      "Training: 100%|██████████| 1442/1442 [14:03<00:00,  1.71it/s, Epoch=7/7, Train Loss=0.0346, Train Acc=98.86%, Val Loss=0.1879, Val Acc=94.65%] \n",
      "Training: 100%|██████████| 903/903 [16:46<00:00,  1.12s/it, Epoch=7/7, Train Loss=0.4273, Train Acc=89.78%, Val Loss=0.4729, Val Acc=88.62%]  \n",
      "Training: 100%|██████████| 2884/2884 [14:46<00:00,  3.25it/s, Epoch=7/7, Train Loss=0.1419, Train Acc=95.84%, Val Loss=0.5118, Val Acc=88.27%]  \n",
      "Training: 100%|██████████| 1442/1442 [14:23<00:00,  1.67it/s, Epoch=7/7, Train Loss=1.3761, Train Acc=82.81%, Val Loss=0.7664, Val Acc=87.97%]  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "87.97083839611179"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(config9)\n",
    "train_model(config10)\n",
    "train_model(config11)\n",
    "train_model(config12)\n",
    "train_model(config13)\n",
    "train_model(config14)\n",
    "train_model(config15)\n",
    "train_model(config16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bd5a86",
   "metadata": {},
   "source": [
    "## Brocoli Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e7930a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 903/903 [14:05<00:00,  1.07it/s, Epoch=7/7, Train Loss=0.5169, Train Acc=81.95%, Val Loss=0.5127, Val Acc=82.05%]  \n",
      "Training: 100%|██████████| 1442/1442 [10:49<00:00,  2.22it/s, Epoch=7/7, Train Loss=2.3982, Train Acc=79.81%, Val Loss=1.1673, Val Acc=86.82%] \n",
      "Training: 100%|██████████| 1806/1806 [09:20<00:00,  3.22it/s, Epoch=7/7, Train Loss=1.2664, Train Acc=85.12%, Val Loss=1.5324, Val Acc=85.55%]  \n",
      "Training: 100%|██████████| 721/721 [06:10<00:00,  1.95it/s, Epoch=7/7, Train Loss=0.5385, Train Acc=81.16%, Val Loss=0.4785, Val Acc=83.11%]\n",
      "Training: 100%|██████████| 2884/2884 [07:06<00:00,  6.76it/s, Epoch=7/7, Train Loss=1.5985, Train Acc=85.78%, Val Loss=2.0990, Val Acc=85.24%]  \n",
      "Training: 100%|██████████| 1442/1442 [07:29<00:00,  3.20it/s, Epoch=7/7, Train Loss=0.5618, Train Acc=82.36%, Val Loss=0.4038, Val Acc=87.79%] \n",
      "Training: 100%|██████████| 903/903 [08:06<00:00,  1.86it/s, Epoch=7/7, Train Loss=0.6510, Train Acc=80.48%, Val Loss=0.6405, Val Acc=82.20%]\n",
      "Training: 100%|██████████| 2884/2884 [07:48<00:00,  6.16it/s, Epoch=7/7, Train Loss=0.6244, Train Acc=83.06%, Val Loss=0.4920, Val Acc=87.06%]  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "87.78857837181044"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(config17)\n",
    "train_model(config18)\n",
    "train_model(config19)\n",
    "train_model(config20)\n",
    "train_model(config21)\n",
    "train_model(config22)\n",
    "train_model(config23)\n",
    "train_model(config24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b455cf96",
   "metadata": {},
   "source": [
    "## Manual Configurations PreTrained 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "29076658",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['Badminton', 'Cricket', 'Tennis', 'Swimming', 'Soccer', 'Wrestling', 'Karate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aad65ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset1(Dataset):\n",
    "    def __init__(self, root_dir, classes, transform=None, is_train=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Directory with all the class folders\n",
    "            classes (list): List of class names (subfolder names)\n",
    "            transform (callable, optional): Optional transform to be applied on a sample\n",
    "            is_train (bool): Whether this is training data or not\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.classes = classes\n",
    "        self.transform = transform\n",
    "        self.is_train = is_train\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n",
    "        self.samples = []\n",
    "\n",
    "        # Default transforms if none provided\n",
    "        if self.transform is None:\n",
    "            if is_train:\n",
    "                self.transform = T.Compose([\n",
    "                    T.Resize(128),\n",
    "                    T.CenterCrop(128), # Resize to 128x128\n",
    "                    T.ToTensor(),\n",
    "                ])\n",
    "            else:\n",
    "                self.transform = T.Compose([\n",
    "                    T.Resize(128),\n",
    "                    T.CenterCrop(128), # Resize to 128x128\n",
    "                    T.ToTensor(),\n",
    "                ])\n",
    "\n",
    "        for idx, cls in enumerate(classes):\n",
    "            class_folder = os.path.join(root_dir, cls)\n",
    "            if not os.path.isdir(class_folder):\n",
    "                continue\n",
    "            for img_name in os.listdir(class_folder):\n",
    "                if img_name.lower().endswith(('jpg', 'jpeg', 'png')):\n",
    "                    img_path = os.path.join(class_folder, img_name)\n",
    "                    self.samples.append((img_path, idx))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx, retry=0):\n",
    "        img_path, label = self.samples[idx]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, label\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {str(e)}\")\n",
    "            if retry < 3:\n",
    "                return self.__getitem__(random.randint(0, len(self)-1), retry=retry+1)\n",
    "            else:\n",
    "                raise RuntimeError(\"Too many failed image loads.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff4b365f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset2(Dataset):\n",
    "    def __init__(self, root_dir, classes, transform=None, is_train=True, split_ratio=0.8, seed=42):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Directory with all the class folders\n",
    "            classes (list): List of class names (subfolder names)\n",
    "            transform (callable, optional): Optional transform to be applied on a sample\n",
    "            is_train (bool): Whether this is training data or not\n",
    "            split_ratio (float): Ratio for training data (default is 0.8)\n",
    "            seed (int): Seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.classes = classes\n",
    "        self.transform = transform\n",
    "        self.is_train = is_train\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n",
    "        self.samples = []\n",
    "\n",
    "        all_samples = []\n",
    "        for idx, cls in enumerate(classes):\n",
    "            class_folder = os.path.join(root_dir, cls)\n",
    "            if not os.path.isdir(class_folder):\n",
    "                continue\n",
    "            for img_name in os.listdir(class_folder):\n",
    "                if img_name.lower().endswith(('jpg', 'jpeg', 'png')):\n",
    "                    img_path = os.path.join(class_folder, img_name)\n",
    "                    all_samples.append((img_path, idx))\n",
    "\n",
    "        # Shuffle and split once\n",
    "        random.seed(seed)\n",
    "        random.shuffle(all_samples)\n",
    "        split_point = int(len(all_samples) * split_ratio)\n",
    "        if is_train:\n",
    "            self.samples = all_samples[:split_point]\n",
    "        else:\n",
    "            self.samples = all_samples[split_point:]\n",
    "\n",
    "        # Set default transforms if not provided\n",
    "        if self.transform is None:\n",
    "            if is_train:\n",
    "                self.transform = T.Compose([\n",
    "                    T.Resize(128),\n",
    "                    T.CenterCrop(128), # Resize to 128x128\n",
    "                    T.ToTensor(),\n",
    "                ])\n",
    "            else:\n",
    "                self.transform = T.Compose([\n",
    "                    T.Resize(128),\n",
    "                    T.CenterCrop(128),\n",
    "                    T.ToTensor(),\n",
    "                ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx, retry=0):\n",
    "        img_path, label = self.samples[idx]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, label\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {str(e)}\")\n",
    "            if retry < 3:\n",
    "                return self.__getitem__(random.randint(0, len(self)-1), retry=retry+1)\n",
    "            else:\n",
    "                raise RuntimeError(\"Too many failed image loads.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "91fba349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_all_but_last_n(model, n=2):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Get all modules with parameters\n",
    "    modules_with_params = [m for m in model.modules() if any(p.requires_grad is False for p in m.parameters())]\n",
    "\n",
    "    # Unfreeze last n modules with parameters\n",
    "    for module in modules_with_params[-n:]:\n",
    "        for param in module.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def print_trainable_params(model):\n",
    "    print(\"Trainable Parameters:\")\n",
    "    total = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            num_params = param.numel()\n",
    "            # print(f\"{name}: {num_params}\")\n",
    "            total += num_params\n",
    "    print(f\"Total Trainable Parameters: {total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a99be882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "    if model_name == \"resnet18\":\n",
    "        resnet18 = models.resnet18(weights='DEFAULT')\n",
    "        resnet18.fc = nn.Linear(resnet18.fc.in_features, 7)  # Change the output layer to match the number of classes\n",
    "        resnet18 = freeze_all_but_last_n(resnet18, 2)  # Freeze all but the last 2 layers\n",
    "        resnet18 = resnet18.to(device)\n",
    "        return resnet18\n",
    "\n",
    "    elif model_name == \"resnet34\":\n",
    "        resnet34 = models.resnet34(weights='DEFAULT')\n",
    "        resnet34.fc = nn.Linear(resnet34.fc.in_features, 7)\n",
    "        resnet34 = freeze_all_but_last_n(resnet34, 2)\n",
    "        resnet34 = resnet34.to(device)\n",
    "        return resnet34\n",
    "    \n",
    "    elif model_name == \"resnet50\":\n",
    "        resnet50 = models.resnet50(weights='DEFAULT')\n",
    "        resnet50.fc = nn.Linear(resnet50.fc.in_features, 7)\n",
    "        resnet50 = freeze_all_but_last_n(resnet50, 2)\n",
    "        resnet50 = resnet50.to(device)\n",
    "        return resnet50\n",
    "\n",
    "    elif model_name == \"resnet101\":\n",
    "        resnet101 = models.resnet101(weights='DEFAULT')\n",
    "        resnet101.fc = nn.Linear(resnet101.fc.in_features, 7)\n",
    "        resnet101 = freeze_all_but_last_n(resnet101, 2)\n",
    "        resnet101 = resnet101.to(device)\n",
    "        return resnet101\n",
    "    \n",
    "    elif model_name == \"resnet152\":\n",
    "        resnet152 = models.resnet152(weights='DEFAULT')\n",
    "        resnet152.fc = nn.Linear(resnet152.fc.in_features, 7)\n",
    "        resnet152 = freeze_all_but_last_n(resnet152, 2)\n",
    "        resnet152 = resnet152.to(device)\n",
    "        return resnet152\n",
    "    \n",
    "    elif model_name == \"vgg16\":\n",
    "        vgg16 = models.vgg16(weights='DEFAULT')\n",
    "        vgg16.classifier[6] = nn.Linear(vgg16.classifier[6].in_features, 7)\n",
    "        vgg16 = freeze_all_but_last_n(vgg16, 1)\n",
    "        vgg16 = vgg16.to(device)\n",
    "        return vgg16\n",
    "    \n",
    "    elif model_name == \"alexnet\":\n",
    "        alexnet = models.alexnet(weights='DEFAULT')\n",
    "        alexnet.classifier[6] = nn.Linear(alexnet.classifier[6].in_features, 7)\n",
    "        alexnet = freeze_all_but_last_n(alexnet, 1)\n",
    "        alexnet = alexnet.to(device)\n",
    "        return alexnet\n",
    "    \n",
    "    elif model_name == \"googlenet\":\n",
    "        googlenet = models.googlenet(weights='DEFAULT')\n",
    "        googlenet.fc = nn.Linear(googlenet.fc.in_features, 7)\n",
    "        googlenet = freeze_all_but_last_n(googlenet, 2)\n",
    "        googlenet = googlenet.to(device)\n",
    "        return googlenet\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Model {model_name} not recognized. Please choose a valid model name.\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a7b96be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(config, transform=None):\n",
    "    dataset_type = config[\"dataset_class\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    seed = 42 \n",
    "\n",
    "    if dataset_type == \"ImageClass1\": # using train and test directories\n",
    "        train_dataset = ImageDataset1(root_dir=train_dir, transform=transform, classes=classes, is_train=True)\n",
    "        val_dataset = ImageDataset1(root_dir=test_dir, transform=transform, classes=classes, is_train=False)\n",
    "    else: # Splitting train into train and validation sets\n",
    "        train_dataset = ImageDataset2(\n",
    "            root_dir=train_dir,\n",
    "            classes=classes,\n",
    "            transform=transform,\n",
    "            is_train=True,\n",
    "            split_ratio=0.8,\n",
    "            seed=seed\n",
    "        )\n",
    "        val_dataset = ImageDataset2(\n",
    "            root_dir=train_dir,\n",
    "            classes=classes,\n",
    "            transform=transform,\n",
    "            is_train=False,\n",
    "            split_ratio=0.8,\n",
    "            seed=seed\n",
    "        )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def validate_model(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    return val_loss / len(val_loader), 100. * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ee0835ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model, method):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "            if method == \"xavier\":\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "            elif method == \"kaiming\":\n",
    "                nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "\n",
    "def should_initialize(model_type):\n",
    "    return model_type == \"scratch\"  # only initialize scratch models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5757be80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def train_model(config):\n",
    "    model_type , model_name = config[\"model_choice\"]\n",
    "    train_loader, val_loader = get_dataloaders(config)\n",
    "\n",
    "    init_method = config[\"init_method\"]\n",
    "\n",
    "    model = load_model(model_name)\n",
    "    model.to(device)\n",
    "    time_stamp = datetime.now().strftime(\"%Y%m%d_%H\")\n",
    "    unique_config = f\"{model_name}_{config['dataset_class']}_{config['optimizer']}_{config['init_method']}_{config['batch_size']}_{config['lr']}_time_{time_stamp}\"\n",
    "\n",
    "    if should_initialize(config[\"model_choice\"][0]) and init_method != \"default\":\n",
    "        initialize_weights(model, init_method)\n",
    "\n",
    "    # Optimizer\n",
    "    if config[\"optimizer\"] == \"adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "    elif config[\"optimizer\"] == \"sgd\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=config[\"lr\"])\n",
    "    else:\n",
    "        optimizer = torch.optim.RMSprop(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "\n",
    "    epochs = 5\n",
    "    save_interval = 2 if model_type == \"pretrained\" else 50\n",
    "    save_dir = os.path.join(\"logs_128_pretrained_center_crop\", \"checkpoints\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    total_batches = len(train_loader)\n",
    "    total_steps = epochs * total_batches\n",
    "    progress_bar = tqdm(total=total_steps, dynamic_ncols=True, desc=\"Training\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            # Update tqdm\n",
    "            train_loss = running_loss / (i + 1)\n",
    "            train_acc = 100. * correct / total\n",
    "            progress_bar.update(1)\n",
    "            progress_bar.set_postfix({\n",
    "                \"Epoch\": f\"{epoch+1}/{epochs}\",\n",
    "                \"Train Loss\": f\"{train_loss:.4f}\",\n",
    "                \"Train Acc\": f\"{train_acc:.2f}%\"\n",
    "            })\n",
    "\n",
    "        # Validation phase\n",
    "        val_loss, val_acc = validate_model(model, val_loader, criterion)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "        # Save model checkpoint\n",
    "        if (epoch + 1) % save_interval == 0:\n",
    "            os.makedirs(os.path.join(save_dir, unique_config), exist_ok=True)\n",
    "            torch.save(model.state_dict(), os.path.join(save_dir, unique_config, f\"epoch_{epoch+1}.pt\"))\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= 20:\n",
    "                progress_bar.set_description(\"Early Stopping\")\n",
    "                break\n",
    "    \n",
    "        progress_bar.set_postfix({\"Epoch\": f\"{epoch+1}/{epochs}\", \"Train Loss\": f\"{train_loss:.4f}\", \"Train Acc\": f\"{train_acc:.2f}%\", \"Val Loss\": f\"{val_loss:.4f}\", \"Val Acc\": f\"{val_acc:.2f}%\"})\n",
    "    progress_bar.close()\n",
    "\n",
    "    # Save metrics\n",
    "    os.makedirs(os.path.join(save_dir, unique_config), exist_ok=True)\n",
    "    torch.save({\n",
    "        \"train_losses\": train_losses,\n",
    "        \"val_losses\": val_losses,\n",
    "        \"train_accs\": train_accs,\n",
    "        \"val_accs\": val_accs\n",
    "    }, os.path.join(save_dir, unique_config, \"metrics.pt\"))\n",
    "\n",
    "\n",
    "    torch.save(model.state_dict(), os.path.join(save_dir, unique_config, \"final_model.pt\"))\n",
    "\n",
    "    return max(val_accs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "186e07e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model(config2)\n",
    "# train_model(config1)\n",
    "# train_model(config3)\n",
    "# train_model(config4)\n",
    "# train_model(config5)\n",
    "# train_model(config6)\n",
    "# train_model(config7)\n",
    "# train_model(config8)\n",
    "# train_model(config9)\n",
    "# train_model(config10)\n",
    "# train_model(config11)\n",
    "# train_model(config12)\n",
    "# train_model(config13)\n",
    "# train_model(config14)\n",
    "# train_model(config15)\n",
    "# train_model(config16)\n",
    "# train_model(config17)\n",
    "# train_model(config18)\n",
    "# train_model(config19)\n",
    "# train_model(config20)\n",
    "# train_model(config21)\n",
    "# train_model(config22)\n",
    "# train_model(config23)\n",
    "# train_model(config24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7cd1f5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [04:05<00:00,  2.09it/s, Epoch=5/5, Train Loss=0.6921, Train Acc=78.21%, Val Loss=0.6962, Val Acc=77.22%]\n",
      "Training: 100%|██████████| 1030/1030 [04:23<00:00,  3.92it/s, Epoch=5/5, Train Loss=0.6380, Train Acc=77.80%, Val Loss=0.8051, Val Acc=75.64%]\n",
      "Training: 100%|██████████| 2575/2575 [05:38<00:00,  7.61it/s, Epoch=5/5, Train Loss=0.7347, Train Acc=74.74%, Val Loss=0.9628, Val Acc=71.74%] \n",
      "Training: 100%|██████████| 1290/1290 [05:41<00:00,  3.77it/s, Epoch=5/5, Train Loss=0.5506, Train Acc=81.07%, Val Loss=0.6352, Val Acc=80.50%]\n",
      "Training: 100%|██████████| 515/515 [03:57<00:00,  2.17it/s, Epoch=5/5, Train Loss=0.6492, Train Acc=78.85%, Val Loss=0.6919, Val Acc=77.95%]\n",
      "Training: 100%|██████████| 1030/1030 [04:35<00:00,  3.74it/s, Epoch=5/5, Train Loss=0.5663, Train Acc=80.09%, Val Loss=0.7011, Val Acc=80.50%]\n",
      "Training: 100%|██████████| 1030/1030 [04:39<00:00,  3.68it/s, Epoch=5/5, Train Loss=0.1778, Train Acc=93.85%, Val Loss=0.4754, Val Acc=88.21%]\n",
      "Training: 100%|██████████| 645/645 [05:13<00:00,  2.05it/s, Epoch=5/5, Train Loss=0.7389, Train Acc=80.84%, Val Loss=0.7724, Val Acc=79.47%]\n",
      "Training: 100%|██████████| 2060/2060 [04:55<00:00,  6.96it/s, Epoch=5/5, Train Loss=0.3113, Train Acc=90.28%, Val Loss=0.3775, Val Acc=87.36%] \n",
      "Training: 100%|██████████| 1030/1030 [04:55<00:00,  3.48it/s, Epoch=5/5, Train Loss=0.3007, Train Acc=90.14%, Val Loss=0.6079, Val Acc=83.84%]\n",
      "Training: 100%|██████████| 1290/1290 [06:07<00:00,  3.51it/s, Epoch=5/5, Train Loss=0.5587, Train Acc=83.41%, Val Loss=0.6443, Val Acc=82.05%]\n",
      "Training: 100%|██████████| 515/515 [04:20<00:00,  1.98it/s, Epoch=5/5, Train Loss=0.2372, Train Acc=92.07%, Val Loss=0.5787, Val Acc=84.75%]\n",
      "Training: 100%|██████████| 1030/1030 [05:06<00:00,  3.36it/s, Epoch=5/5, Train Loss=0.2679, Train Acc=91.35%, Val Loss=0.6543, Val Acc=83.78%]\n",
      "Training: 100%|██████████| 645/645 [06:02<00:00,  1.78it/s, Epoch=5/5, Train Loss=0.6335, Train Acc=82.02%, Val Loss=0.6815, Val Acc=80.30%]\n",
      "Training: 100%|██████████| 2060/2060 [05:22<00:00,  6.39it/s, Epoch=5/5, Train Loss=0.5543, Train Acc=86.64%, Val Loss=1.0494, Val Acc=82.38%] \n",
      "Training: 100%|██████████| 1030/1030 [05:07<00:00,  3.34it/s, Epoch=5/5, Train Loss=2.9589, Train Acc=75.17%, Val Loss=1.8182, Val Acc=81.47%]\n",
      "Training: 100%|██████████| 645/645 [05:38<00:00,  1.90it/s, Epoch=5/5, Train Loss=0.6678, Train Acc=75.91%, Val Loss=0.6173, Val Acc=79.43%]\n",
      "Training: 100%|██████████| 1030/1030 [05:12<00:00,  3.29it/s, Epoch=5/5, Train Loss=4.0650, Train Acc=73.38%, Val Loss=3.3198, Val Acc=77.58%]\n",
      "Training: 100%|██████████| 1290/1290 [05:18<00:00,  4.05it/s, Epoch=5/5, Train Loss=1.7911, Train Acc=82.17%, Val Loss=2.2616, Val Acc=81.03%]\n",
      "Training: 100%|██████████| 515/515 [04:02<00:00,  2.13it/s, Epoch=5/5, Train Loss=0.7804, Train Acc=74.52%, Val Loss=0.7045, Val Acc=77.16%]\n",
      "Training: 100%|██████████| 2060/2060 [04:38<00:00,  7.39it/s, Epoch=5/5, Train Loss=2.6627, Train Acc=82.80%, Val Loss=3.8625, Val Acc=80.50%]\n",
      "Training: 100%|██████████| 1030/1030 [04:27<00:00,  3.85it/s, Epoch=5/5, Train Loss=1.0381, Train Acc=70.38%, Val Loss=0.8225, Val Acc=76.31%]\n",
      "Training: 100%|██████████| 645/645 [04:51<00:00,  2.21it/s, Epoch=5/5, Train Loss=0.9424, Train Acc=70.60%, Val Loss=0.9394, Val Acc=71.25%]\n",
      "Training: 100%|██████████| 2060/2060 [04:38<00:00,  7.40it/s, Epoch=5/5, Train Loss=1.2074, Train Acc=69.49%, Val Loss=1.0445, Val Acc=73.63%] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "73.63304981773997"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(config2)\n",
    "train_model(config1)\n",
    "train_model(config3)\n",
    "train_model(config4)\n",
    "train_model(config5)\n",
    "train_model(config6)\n",
    "train_model(config7)\n",
    "train_model(config8)\n",
    "train_model(config9)\n",
    "train_model(config10)\n",
    "train_model(config11)\n",
    "train_model(config12)\n",
    "train_model(config13)\n",
    "train_model(config14)\n",
    "train_model(config15)\n",
    "train_model(config16)\n",
    "train_model(config17)\n",
    "train_model(config18)\n",
    "train_model(config19)\n",
    "train_model(config20)\n",
    "train_model(config21)\n",
    "train_model(config22)\n",
    "train_model(config23)\n",
    "train_model(config24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6d65bf",
   "metadata": {},
   "source": [
    "# Functions to load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "54fc137c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_weights(model, path):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    return model\n",
    "\n",
    "def load_metrics(path):\n",
    "    return torch.load(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
